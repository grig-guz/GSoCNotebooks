{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing Base Model\n",
    "\n",
    "+ **Goal**: Using the [LibriVox dataset](http://www.openslr.org/12), located in `data/base/raw/test-clean` and `data/base/raw/train-clean`, pre-process the data into images (spectrograms) and output to `data/base/process/train-clean` and `data/base/process/test-clean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svmem(total=13660872704, available=6186033152, percent=54.7, used=7124361216, free=211632128, active=12238913536, inactive=1002651648, buffers=54595584, cached=6270283776, shared=52719616)\n"
     ]
    }
   ],
   "source": [
    "#!pip install psutil\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import librosa.display\n",
    "from matplotlib import cm\n",
    "from sklearn import preprocessing\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "import psutil\n",
    "\n",
    "rootdir = \"/nb/transfer/data/base/raw/train-clean-100\"\n",
    "PROCESSED_DIR = '/nb/transfer/data/base/'\n",
    "samples_per_observation = 16000 # 1-second windows (given librivox corpus)\n",
    "print(psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "sr = 16000\n",
    "## We can choose any colormap we want https://matplotlib.org/examples/color/colormaps_reference.html\n",
    "convert = plt.get_cmap(cm.jet)\n",
    "\n",
    "def get_spectrograms(rows):\n",
    "    imgs = np.zeros((rows.shape[0], 1025, 32, 3))\n",
    "    for i in range(0, rows.shape[0]):\n",
    "        X = librosa.stft(rows[i])\n",
    "        Xdb = librosa.amplitude_to_db(X)\n",
    "        Xdb = min_max_scaler.fit_transform(Xdb)\n",
    "        numpy_output_static = convert(Xdb)[:,:,:3]\n",
    "        #Somehow, convert returns the spectrograms flipped upside down, so we need to fix that \n",
    "        numpy_output_static = np.flip(numpy_output_static, 0)\n",
    "        imgs[i] = numpy_output_static\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading audio\n",
    "\n",
    "First we extract the numpy arrays from the audio itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_observations(signal, label):\n",
    "    \"\"\"\n",
    "    Create two numpy arrays: observations and labels.\n",
    "    \n",
    "    Args:\n",
    "        signal: An np arry with shape (1, num_samples)\n",
    "        label: The label of the speaker shape (1, num_samples)\n",
    "    \"\"\"\n",
    "    num_samples = signal.shape[0]\n",
    "    truncated = signal[:-(num_samples % samples_per_observation)]\n",
    "    truncated = truncated.reshape((num_samples // samples_per_observation, -1))\n",
    "    labels = np.zeros((truncated.shape[0], 1), dtype=np.int)\n",
    "    labels[:] = label\n",
    "    return truncated.reshape((num_samples // samples_per_observation, -1)), labels\n",
    "\n",
    "def build_rows(path, speaker_label):\n",
    "    signal, sample_rate = sf.read(path)\n",
    "    observations, labels = extract_observations(signal, speaker_label)\n",
    "    return observations, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID SEX           SUBSET  MINUTES              NAME\n",
      "0  14   F  train-clean-360    25.03   Kristin LeMoine\n",
      "1  16   F  train-clean-360    25.11    Alys AtteWater\n",
      "2  17   M  train-clean-360    25.04    Gord Mackenzie\n",
      "3  19   F  train-clean-100    25.19  Kara Shallenberg\n",
      "4  20   F  train-other-500    30.07            Gesine\n",
      "Number of females: 1201\n",
      "Number of males: 1283\n"
     ]
    }
   ],
   "source": [
    "## Just figuring out male/female ratio in our dataset\n",
    "df = pd.read_csv(\n",
    "    '/nb/transfer/data/base/raw/SPEAKERS.TXT',\n",
    "    skiprows=12,\n",
    "    sep='\\s+\\|\\s+',\n",
    "    names=['ID', 'SEX', 'SUBSET', 'MINUTES', 'NAME'],\n",
    "    engine='python')\n",
    "print(df.head())\n",
    "males = df[df.SEX == 'M']\n",
    "females = df[df.SEX == 'F']\n",
    "print(\"Number of females: \" + str(len(females)))\n",
    "print(\"Number of males: \" + str(len(males)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c33a2ba16baf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m                         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                     \u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_spectrograms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                     \u001b[0mnfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                     \u001b[0mnlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_data_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m## Add this speaker to our current chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "## Features and labels of current dataset chunk\n",
    "features = None\n",
    "labels = None\n",
    "\n",
    "## Features and labels of current speaker\n",
    "nfeatures = None \n",
    "nlabels = None\n",
    "\n",
    "speaker_label = 0\n",
    "chunk_count = 100\n",
    "speakers_per_chunk = 5\n",
    "oppos = {\n",
    "    'M':'F',\n",
    "    'F':'M'\n",
    "}\n",
    "current = 'F'\n",
    "\n",
    "\n",
    "for speaker in os.listdir(rootdir):\n",
    "    # Each speaker has multiple books which he/she read\n",
    "    print(int(speaker))\n",
    "    # Ugly but works ok. Skipping speakers if they have the same gender as previous\n",
    "    # processed speaker. If so, we don't add them to keep gender ratio\n",
    "    if(df.loc[df['ID'] == int(speaker)].SEX.item() == oppos[current]):\n",
    "        current = oppos[current]\n",
    "    else:\n",
    "        continue\n",
    "    speaker_path = os.path.join(rootdir, speaker)\n",
    "    for book_id in os.listdir(speaker_path):\n",
    "        # Going over all audiosampes for a given book\n",
    "        book_id_path = os.path.join(speaker_path, book_id)\n",
    "        for file in os.listdir(book_id_path):\n",
    "            # Sometimes there are .txt files in audiosamples' folders\n",
    "            if (not file.endswith('.txt')):\n",
    "                if (nfeatures is None):\n",
    "                    ndata, nlabels = build_rows(os.path.join(book_id_path, file), speaker_label)\n",
    "                    ## If size of the audiofile is less than samples_per_observation\n",
    "                    if (ndata.shape[1] == 0):\n",
    "                        continue\n",
    "                    nfeatures = get_spectrograms(ndata)\n",
    "                else:\n",
    "                    ndata, n_data_labels = build_rows(os.path.join(book_id_path, file), speaker_label)\n",
    "                    ## If size of the audiofile is less than samples_per_observation\n",
    "                    if (ndata.shape[1] == 0):\n",
    "                        continue\n",
    "                    imgs = get_spectrograms(ndata)\n",
    "                    nfeatures = np.vstack((nfeatures, imgs))\n",
    "                    nlabels = np.vstack((nlabels, n_data_labels))\n",
    "    ## Add this speaker to our current chunk\n",
    "    if (features is None):\n",
    "        features = nfeatures\n",
    "        labels = nlabels\n",
    "    else:\n",
    "        features = np.vstack((features, nfeatures))\n",
    "        labels = np.vstack((labels, nlabels))\n",
    "    ## We mess with all features and nfeatures that because vstack reassables whole array, so if we already have data in features array for\n",
    "    ## multiple speakers, vstacking by 1 row will take painfully long time. \n",
    "    ## add \n",
    "    nfeatures = None\n",
    "    nlabels = None\n",
    "    \n",
    "    speaker_label += 1\n",
    "    print(features.shape)\n",
    "    print(labels.shape)\n",
    "    ## Save speaker_per_chunk speakers' spectrograms into a chunk\n",
    "    if (speaker_label % speakers_per_chunk == 0):\n",
    "        print(features.shape)\n",
    "        print(labels.shape)\n",
    "        # Permute the given chunk\n",
    "        permutation = np.random.permutation(features.shape[0])\n",
    "        features = np.take(features, permutation,axis=0)\n",
    "        labels = np.take(labels, permutation, axis=0)\n",
    "        \n",
    "        # Save data, start assembling new chunk\n",
    "        np.save(os.path.join(PROCESSED_DIR, 'chunk_%d_features' % chunk_count), features)\n",
    "        np.save(os.path.join(PROCESSED_DIR, 'chunk_%d_labels' % chunk_count), labels)\n",
    "        chunk_count += 1\n",
    "        features = None\n",
    "        labels = None\n",
    "    if (speaker_label == 50):\n",
    "        break\n",
    "    print('Speaker %d processed' % speaker_label)\n",
    "    print(psutil.virtual_memory()) ## uncomment if you want to view the memory usage\n",
    "print(\"Dataset building finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving To HDF5\n",
    "\n",
    "The training data is larger than memory, we'd like to transfer everything to one HDF5 file, but we need to do it in chunks. See the `batcher.py` file for how we make this happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from batcher import TrainingBatcher\n",
    "\n",
    "ROOT_DIR = '/nb'\n",
    "TRANSFER_DIR = os.path.join(ROOT_DIR, 'transfer')\n",
    "DATA_DIR = os.path.join(TRANSFER_DIR, 'data')\n",
    "BASE_DIR = os.path.join(DATA_DIR, 'base')\n",
    "PROCESSED_DIR = os.path.join(BASE_DIR, 'processed')\n",
    "HDF5_FILE = os.path.join(PROCESSED_DIR, 'train.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write To File\n",
    "\n",
    "Executing the below code will write all numpy files to `HDFS_FILE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7222, 1025, 32, 3)\n",
      "(7222, 1)\n",
      "(7222, 1025, 32, 3)\n",
      "(6860, 1025, 32, 3)\n",
      "(6860, 1)\n",
      "(14082, 1025, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "numpy_files = defaultdict(list)\n",
    "\n",
    "for f in glob.glob(os.path.join(PROCESSED_DIR, '*.npy')):\n",
    "    numpy_files[int(re.findall(r'\\d+', f)[0])].append(f)\n",
    "\n",
    "with TrainingBatcher(HDF5_FILE, 'w') as data:\n",
    "    for key, l in numpy_files.items():\n",
    "        X = np.load([i for i in l if 'features' in i][0])\n",
    "        y = np.load([i for i in l if 'labels' in i][0])\n",
    "        print(X.shape)\n",
    "        print(y.shape)\n",
    "        data.write(X, y)\n",
    "        print(data.X_ds.shape)\n",
    "        del X\n",
    "        del y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
